{"cells":[{"cell_type":"markdown","id":"88519e1d","metadata":{"id":"88519e1d"},"source":["This week, we’ll get to use the spaCy python library for the first time. SpaCy is designed to make it\n","easy to use pre-trained models to analyze very large sets of data. At the beginning of the semester,\n","we’ll be using spaCy as a skeleton for building our own NLP algorithms.\n","\n","Install SpaCy and import it to verify that it is working.\n","\n","We will use SpaCy to tokenize our text. Pause now and read about space language processing\n","pipelines.\n","\n","For this part, we only want SpaCy to tokenize our text, so we will set the pipeline to []. Since\n","some of our documents will be long, but since we’re not doing any memory-intensive processing,\n","we will tell SpaCy that it’s okay to load large documents all at once instead of a little bit at a time.\n","To specify these instructions, add these lines to the top of your zipf.py file:\n","```python\n","from spaCy.lang.en import English\n","\n","nlp = English(pipeline=[], max_length=5000000)\n","```\n"]},{"cell_type":"code","execution_count":1,"id":"5fed29c2","metadata":{"id":"5fed29c2","executionInfo":{"status":"ok","timestamp":1684494106562,"user_tz":-420,"elapsed":8930,"user":{"displayName":"Quân Tiến Quân","userId":"08559202489371626776"}}},"outputs":[],"source":["import requests\n","import io\n","import re\n","from spacy.lang.en import English\n","nlp = English(pipeline=[], max_length=5000000)"]},{"cell_type":"code","execution_count":2,"id":"661f9b16","metadata":{"id":"661f9b16","executionInfo":{"status":"ok","timestamp":1684494106562,"user_tz":-420,"elapsed":49,"user":{"displayName":"Quân Tiến Quân","userId":"08559202489371626776"}}},"outputs":[],"source":["url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n","text = requests.get(url).content.decode('utf8')\n"]},{"cell_type":"code","execution_count":3,"id":"9be490ca","metadata":{"id":"9be490ca","executionInfo":{"status":"ok","timestamp":1684494106563,"user_tz":-420,"elapsed":47,"user":{"displayName":"Quân Tiến Quân","userId":"08559202489371626776"}}},"outputs":[],"source":["def clean_text(text):\n","    filters='\\t\\n'\n","    translate_dict = dict((c, \" \") for c in filters)\n","    translate_map = str.maketrans(translate_dict)\n","    text = text.translate(translate_map)\n","    text = re.sub(' +', ' ', text)\n","    return text\n","text=clean_text(text)"]},{"cell_type":"code","execution_count":4,"id":"c51042ae","metadata":{"id":"c51042ae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684494106563,"user_tz":-420,"elapsed":47,"user":{"displayName":"Quân Tiến Quân","userId":"08559202489371626776"}},"outputId":"daa698ef-c1d8-459c-98aa-ae3a4e64569f"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Language is never, ever, ever, random ADAM KILGARRIFF Abstract Language users never choose words randomly, and language is essentially non-random. Statistical hypothesis testing uses a null hypothesis, which posits randomness. Hence, when we look at linguistic phenomena in cor- pora, the null hypothesis will never be true. Moreover, where there is enough data, we shall (almost) always be able to establish that it is not true. In corpus studies, we frequently do have enough data, so the fact that a rela- tion between two phenomena is demonstrably non-random, does not sup- port the inference that it is not arbitrary. We present experimental evidence of how arbitrary associations between word frequencies and corpora are systematically non-random. We review literature in which hypothesis test- ing has been used, and show how it has often led to unhelpful or mislead- ing results. Keywords: 쎲쎲쎲 1. Introduction Any two phenomena might or might not be related. The range of pos- sibilities is that the association is Random, Arbitrary, Motivated or Pre- dictable (R, A, M, P). The bulk of linguistic questions concern the dis- tinction between A and M. A linguistic account of a phenomenon gen- erally gives us reason to view the relation between, for example, a verb’s syntax and its semantics, as motivated rather than arbitrary. However, it is not in general possible to model the A-M distinction mathematically. The distinction that can be modeled mathematically is between R and not-R, that is, between random, or uncorrelated, pairs and pairs where there is some correlation, be it arbitrary, motivated or predictable.1 The mechanism here is hypothesis-testing. A null hypothesis, H0 is con- structed to model the situation in which there is no correlation between Corpus Linguistics and Linguistic Theory 1⫺2 (2005), 263⫺275 1613-7027/05/0001⫺0263 쑕 Walter de Gruyter \f264 A. Kilgarriff the two phenomena. As the mathematics of the random is well under- stood, we can compute the likelihood of the null hypothesis given the data. If the likelihood is low, we reject H0. The problem for empirical linguistics is that language is not random, so the null hypothesis is never true. Language is not random because we speak or write with purposes. We do not, indeed, without computational help are not capable of, producing words or sounds or sentences or documents randomly. We do not always have enough data to reject the null hypothesis, but that is a distinct issue: wherever there is enough data, it is rejected. Using language corpora, we are frequently in the fortunate position of having very large quantities of data at our disposal. Then, even where pairs of corpora are set up to be linguistically identical, the null hypothesis is resoundingly defeated. In section 4, we present an experiment demonstrating this counterintuitive effect. There are a number of papers in the empirical linguistics literature where researchers seemed to be testing whether an association was lin- guistically salient, or used the confidence with which H0 could be re- jected as a measure of salience, whereas in fact they were merely testing whether they had enough data to reject H0 with confidence. Some such cases are reviewed in section 5. Hypothesis testing has been widely used in the acquisition of subcategorization frames from corpora and this literature is considered in some detail. Alternatives to inappropriate hy- pothesis-testing are presented. Before proceeding, may I clarify that this paper is in no way critical of using probability models, all of which are based on assumptions of randomness, in empirical linguistics in general. Probability models have been responsible for a large share of progress in the field in the last decade and a half. The randomness assumptions are always untrue, but that does not preclude them from frequently being useful. Making false assumptions is often an ingenious way to proceed; the problem arises where the literal falsity of the assumption is overlooked, and inappropri- ate inferences are drawn. 2. The arbitrary and the random In common parlance, random and arbitrary are synonyms, with diction- aries giving near-identical definitions: LDOCE (1995) defines random as happening or chosen without any definite plan, or pattern and arbitrary as 1 decided or arranged without any reason or plan, often unfairly … 2 happening or decided by chance rather than a plan \f Language is never, ever, ever, random 265 Superficially, randomness, as defined here, is what the technical sense of random captures and makes explicit. The technical sense is defined in terms of statistical independence. First, we formalize the framework: For a population of events, the first phenomenon holds where x is true of the event, the second holds where y is true of the event. Now, the relation between the phenomena is random iff the prob- ability of x, for that subset of events where y does hold, is identical to its probability for the subset where y does not hold, that is P (x|y) ⫽ P (x|ÿ y) The relation is symmetric: P (x|y) ⫽ P (x|ÿ y) entails P (y|x) ⫽ P (y|ÿ x). Hereafter I use ‘random’ for the technical meaning and ‘arbi- trary’ for the non-technical one. Arbitrary events are very rarely random, and random events are very rarely arbitrary. It takes considerable ingenuity and sophisticated mathe- matics to produce a pseudo-random sequence algorithmically, and true randomness is not possible at all. Events happening “without any defi- nite plan, aim or pattern” are, by definition, arbitrary, but are vanish- ingly unlikely to be random. Outside the sub-atomic realm, natural events are very rarely random. Consider, for example, cat food purchases and shoe-polish purchases within the space of all UK supermarket-shopping events: does the fact that cat food was bought predict (positively or negatively) whether shoe polish was bought in the same shopping trip? There is no obvious reason why it should, and we can happily declare the relation arbitrary. But perhaps either cat food or shoe-polish are more (or less) often bought in hot (or cold) weather, or on Saturday nights, or Sunday mornings, or Monday lunchtimes, or by richer (or poorer) people, or by men (or women), or by people in (or out of) towns… There is an unlimited number of hypotheses connecting the two (positively or negatively); if just one of these has any validity, however weak, then the null hypothesis is false. At this point, you may question why the null hypothesis is ever a useful construct. For a wide range of tasks, although H0 is false, there is only enough evidence to establish the fact if there is a strong relation between the two phenomena. Thus, given evidence from 1,000 shopping trips, it is un- likely we shall be able to reject H0 concerning cat food and shoe-polish, whereas we shall be able to reject it concerning strawberry-buying and cream-buying. Given further evidence, perhaps from 1,000,000 shopping \f266 A. Kilgarriff trips, we shall also be able to reject the null hypothesis regarding nappy2- buying and beer-sixpack-buying. (The correlation, the most newsworthy product of large-scale data mining by supermarkets, was widely reported in the British media.) But still not for catf ood and shoe-polish. But, given 1,000,000,000 events, we shall in all likelihood also be able to reject it for cat food and shoe-polish. Whether or not we can reject the null hypothesis (with eg. 95 % confi- dence) is a function of sample size and level of correlation. Where sample size is held constant (and is not enormous), whether or not we can reject H0 can be seen as a way of providing statistical support for distinguish- ing the arbitrary and the motivated. This is a role that hypothesis testing plays across the social sciences. However where the sample size varies by an order of magnitude, or where it is enormous, it is wrong to identify the accept-H0/reject-H0 distinction with the arbitrary/motivated one. The uneasy relationship between hypothesis-testing, and quantity of data, is familiar to statisticians though frequently overlooked or mis- understood by users of statistics (Carver 1993, Stubbs 1995, Brandstätter 1999). One statistics textbook warns thus: None of the null hypotheses we have considered with respect to good- ness of fit can be exactly true, so if we increase the sample size (and hence the value of χ2) we would ultimately reach the point when all null hypotheses would be rejected. All that the χ2 test can tell us, then, is that the sample size is too small to reject the null hypothesis! (Owen and Jones, 1977, p 359). The issue is particularly salient for empirical linguistics because, firstly, we have access to extremely large sample sizes, and secondly, the distri- bution of many language phenomena is Zipfian. The has 6,000,000 oc- currences in the BNC whereas cat food (spelled as one word or two) has 66. For a vast number of third phenomena X, the null hypothesis that the and X are uncorrelated will be rejected, whereas the null hypothesis that cat food and X are uncorrelated will not. It would be wrong to draw inferences about what was arbitrary, what motivated. 3. Objections to Maximum Likelihood Estimates (MLEs) Church and Hanks (1990) inaugurated the research area of lexical statis- tics with their presentation of Mutual Information (I), a measure of how closely associated two phenomena are. It can be applied to finding words which occur together to a noteworthy degree, or to finding words which are particularly associated with one corpus as against another, or for various other purposes.3 They define the mutual information between two words x and y as \f Language is never, ever, ever, random 267 I (x; y) ⫽ log 2 冉 p (xandy) p (x) · p(y) 冊 and then estimate probabilities directly from frequencies, that is using the ‘Maximum Likelihood Estimate’ (MLE) of f (x)/N for p (x), f (y)/N for p (y), f (x-and-y)/N for p (x-and-y), thereby giving I (x; y) ⫽ log 2 冉 N · f (x ⫺ and ⫺ y) f (x) · f (y) 冊 Dunning (1993) presents a critique of the use of Mutual Information in empirical linguistics. His objection has been confused with the critique of hypothesis-testing I make here so I mention his work in order to clarify that the two objections, while both valid, are different in nature and independent. Dunning demonstrates how MLEs fare poorly when estimating the probabilities of rare events. The problem is essentially this: if a word (or bigram, or trigram, or character-sequence etc.) occurs just once or twice in a corpus of N words (bigrams, etc.), then the simplest way to estimate the probability is the NILE, which gives 1/N or 2/N. However this does not factor in the arbitrariness of the word occurring at all in the corpus: in a corpus ten times the size, there would be roughly ten times the number of singletons and doubletons in the corpus, most of which would not have occurred at all in the original corpus. Thus some of the prob- ability mass contributing to the 1/N or 2/N MLEs should have been put aside for the words (bigrams etc.) which did not occur at all in this particular corpus. Viewed another way, the 1/N and 2/N should be dis- counted to allow for the fact that one or two occurrences are very low bases of evidence on which to assert probabilities. There are various ways in which the discounting can be done, for example the Good-Turing method (Good 1953), usefully applied to em- pirical linguistics in Gale and Sampson (1995), Bod (1995). Dunning presents and advocates the use of the log-likelihood statistic, which, like the χ2 statistic, is χ2-distributed,4 but more accurately estimates probabil- ities where counts are low. The log-likelihood statistic still only estimates probabilities: since Dunning’s work, Pedersen (1996) has shown how Fisher’s Exact Method can be applied to the problem, to identify the exact probability of a word (bigram etc.) rather than estimating it at all. Thus Dunning’s objection to Mutual Information is that it fails to accurately represent probabilities when counts are low (where ‘low’ is generally taken as less than five). If the probabilities can be accurately represented, Dunning’s anxieties will be set at ease. \f268 A. Kilgarriff The critique in this paper does not concern whether probabilities are accurately calculated. Rather, the objection is that the probability model, with its assumptions of randomness, is inappropriate, particularly where counts are high (eg, thousands or more). Where the task is to determine whether there is an interesting associa- tion between two rare events, Dunning’s concern must be heeded. Where it is to determine whether there is an interesting association between high-frequency events, the concerns of this paper must be. 4. Experiment Given enough data, H0 is almost always rejected however arbitrary the data, as the author discovered when grappling with the following data. Two corpora were set up to be indisputably of the same language type, with only arbitrary differences between them: each was a random subset of the written part of the British National Corpus (BNC). The sampling was as follows: all texts shorter than 20,000 words were excluded. This left 820 texts. Half the texts were then randomly assigned to each of two corpora. The null hypotheses were (1) that the two subcorpora, viewed as col- lections of words rather than documents, were random samples drawn from the same population; and consequently, (2) that the deviation in frequency of occurrence for each individual word between the two sub- corpora was explicable as random fluctuation. The HO were tested using the χ2-test: is χ2 Σ(|O ⫺ E | ⫺ 0.5) 2 /E greater than the critical value? The sum is over the four cells of the contingency table Corpus 1 Corpus 2 word w a b not word w c d If we randomly assign words (as opposed to documents) to the one corpus or the other, then we have a straightforward random distribution, with the value of the χ2-statistic equal to or greater than the 99.5 % confidence threshold of 7.88 for just 0.5 % of words. The average value of the error term, \f Language is never, ever, ever, random 269 (|O ⫺ E| ⫺ 0.5) 2 /E is then 0.5.5 The hypothesis can, therefore, be couched as: are the error terms systematically greater than 0.5? If they are, we should be wary of attributing high error terms to significant differences between text types, since we also obtain many high error terms where there are no significant differences between text types. Frequency lists for word-POS pairs for each subcorpus were gener- ated. For each word occurring in either subcorpus, the error term which would have contributed to a χ2 calculation was determined. As Table 4 shows, average values for the error term are far greater than 0.5, and tend to increase as word frequency increases. As the averages indicate, the error term is very often greater than 0.5 * 7.88 ⫽ 3.94, the relevant critical value of the chi-square statistic. For very many words, including most common words, the null hypothesis is resoundingly defeated (as is the null hypthesis regarding the two subc- orpora as wholes). There is no a priori reason to expect words to behave as if they had been selected at random, and indeed they do not. It is in the nature of Table 1. Comparing two same-genre corpora using χ2 Class First item in class Mean error term (Words in freq. order) for items in class word POS First 10 items the DET 18.76 Next 10 items for PRP 17.45 Next 20 items not XX 14.39 Next 40 items have VHB 10.71 Next 80 items also AVO 7.03 Next 160 items know VVI 6.40 Next 320 items six CRD 5.30 Next 640 items finally AV0 6.71 Next 1280 items plants NN2 6.05 Next 2560 items pocket NN1 5.82 Next 5120 items represent VVB 4.53 Next 10240 items peking NP0 3.07 Next 20480 items fondly AV0 1.87 Next 40960 items chandelier NN1 1.15 Table note. Mean error term is far greater than 0.5, and increases with frequency. POS tags are drawn from the CLAWS-5 tagset as used in the BNC (see http:/info.ox. ac.uk/bnc) \f270 A. Kilgarriff language that any two collections of texts, covering a wide range of registers (and comprising, say, less than a thousand samples of over a thousand words each) will show such differences. While it might seem plausible that oddities would in some way balance out to give a popula- tion that was indistinguishable from one where the individual words (as opposed to the texts) had been randomly selected, this turns out not to be the case. The key word in the last paragraph is ‘indistinguishable’. In hypothesis testing, the objective is generally to see if the population can be distin- guished from one that has been randomly generated ⫺ or, in our case, to see if the two populations are distinguishable from two populations which have been randomly generated on the basis of the frequencies in the joint corpus. Since words in a text are not random, we know that our corpora are not randomly generated, and the hypothesis test con- firms the fact. 5. Re-analysis of previous work 5.1. Brown and LOB Hofland and Johansson (1982) wanted to find words which were signifi- cantly different in their frequencies between British and American Eng- lish, as represented in the Brown corpus for American English and LOB corpus for British. For each word, they tested the null hypothesis that the difference in frequency between the two corpora could be explained as random variation, with the samples being random samples from the same source, and in their frequency lists, they mark words where the null hypothesis was defeated (at a 95, 99 or 99.9 % confidence level). Looking at these lists suggests that virtually all common words are markedly different in their levels of use between the US and the UK: they are all marked as such. By contrast, most of the rarer marked words are words we know to be American or British, or to refer to items that are more common or more salient in the US or the UK. As the argument of the previous section explains, most of the marked high-frequency words are marked simply as a consequence of the essen- tially non-random nature of language. It would not be surprising for a high-frequency word marked as British English in these lists to be marked as American English in a repeat of the experiment using new data. Similar strategies are used by, and a similar critique is applicable to, Leech and Fallon (1992) (again, for comparing LOB and Brown), Ray- \f Language is never, ever, ever, random 271 son, Leech, and Hodges (1997) for comparing the conversation of dif- ferent social groups, and Rayson and Garside (2000) for contrasting the language of a specialist genre with ‘general language’, as represented by the British National Corpus. 5.2 Subcategorization frame (SCF) learning Hypothesis-testing has been used in a number of papers concerning the automatic acquisition of subcategorization frames (SCFs) for verbs from corpora. The problem is this. Dictionaries, even where they do present explicit and accurate SCFs for verbs, are not complete: they do not pre- sent all the frames for each verb. This gives rise to many parsing errors. Researchers including Brent (1993), Briscoe and Carroll (1997) and Kor- honen (2000) have developed methods for SCF acquisition. However, their methods are inevitably noisy, suffering, for example, from just those parser errors that the whole process is designed to address, and they do not wish to accept any SCF for which there is any evidence as a true SCF for the verb. They wish to filter out those SCFs where the evidence is not strong enough. Brent and Briscoe and Carroll used hypothesis testing to this end. However, problems are noted: Further evaluation of the results ...reveals that the filtering phase is the weak link in the system … The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more then 10 exemplars would have pro- duced broadly similar results for these verbs (Briscoe and Carroll 1997: 360⫺36). Korhonen, Correll, and McCarthy (2000) explore the issue in detail. Using Briscoe and Carroll’s SCF acquisition system, they explore the impact of four different strategies for filtering out noise: Baseline No filter BHT binomial hypothesis test: reject the SCF if Ho is not defeated6 LLR hypothesis test using log-likelihood ratio: reject the SCF if Ho is not defeated MLE threshold based on the relative frequency (which is also the maximum likelihood estimate (MLE) of the probability) of the verb occurring in the SCF given the verb, with the thresh- old determined empirically \f272 A. Kilgarriff They observe MLE thresholding produced better results than the two statistical tests used. Precision improved considerably, showing that the classes occur- ring in the data with the highest frequency are often correct … MLE is not adept at finding low frequency SCFs … (Korhonen, Correll, and McCarthy 2000: 202) This concurs with the theoretical argument above. Hypothesis tests are inappropriate for the task, because the relations between verb and SCF will never be random and the hypothesis test will merely reject the null hypothesis wherever there is enough data, in a manner not closely corre- lated with whether the SCF-verb link is motivated. Where there is enough data, then the relationship between verb and SCF is easy to see so even a simple threshold method will identify the verb’s SCFs. Where data is very sparse, no method works well. Korhonen (2000) extends this line of work, exploring thresholding methods where a more accurate estimate of the probability is obtained by using data from semantically similar but higher frequency verbs. She achieves modest improvements over the baseline which uses Korhonen, Correll and McCarthy’s MLE, particularly when combining the fre- quencies of the target verb and its semantic neighbour using a linear method based on the quantity of evidence available for each. The problem is not one of distinguishing random and non-random relationships, but of sparseness of data. Where the data is not sparse, the difference between arbitrary and motivated connections is evident in greatly differing relative frequencies. This makes the moral of the story plain. Data is abundant. A modest-frequency verb like devastate occurs (Google tells us) in well over a million web pages. With just 1 % of them, devastate becomes one of the verbs for which we have plenty of data, and crude thresholding methods will distinguish associated SCFs from noise. It is possible that parsing errors are systematic and thus that the same errors occur very often in very large corpora although our experi- ence from looking at large corpora in the Word Sketch Engine (Kilgarriff et al 2004) suggests not. Harvesting the web (or other huge corpora) is the way to build an accurate SCF lexicon.7 6. Conclusion Language is non-random and hence, when we look at linguistic phenom- ena in corpora, the null hypothesis will never be true. Moreover, where there is enough data, we shall (almost) always be able to establish that it is not true. In corpus studies, we frequently do have enough data, so \f Language is never, ever, ever, random 273 the fact that a relation between two phenomena is demonstrably non- random, does not support the inference that it is not arbitrary. Hypoth- esis testing is rarely useful for distinguishing associated from non-associ- ated pairs of phenomena in large corpora. Where used, it has often led to unhelpful or misleading results. Hypothesis testing has been used to reach conclusions, where the diffi- culty in reaching a conclusion is caused by sparsity of data. But language data, in this age of information glut, is available in vast quantities. A better strategy will generally be to use more data Then the difference between the motivated and the arbitrary will be evident without the use of compromised hypothesis testing. As Lord Rutherford put it: “If your experiment needs statistics, you ought to have done a better experi- ment.” Received July 2004 Lexical Computing Ltd. Revisions received May 2005 Final acceptance May 2005 Notes * This work was supported by the UK EPSRC, under grants GR/K18931 (SEAL) and GR/M54971 (WASPS). 1. In this paper we do not consider the distinction between the predictable and the ‘merely’ motivated. 2. Diapers, in American English 3. There is some confusion over names. In information theory, Mutual Information is usually defined over a whole population of words, rather than being specified for a particular word-pair, as here, and the definition incorporates information from all cells of the contingency table. Church and Hanks only use a subset of that information. Church-and-Hanks Mutual Information has been called Pointwise Mutual Information. see Manning and Schütze (1999: 66 ff.) for a fuller discus- sion. Here we use Church and Hanks’s definition and name. 4. This sentence will be confusing to non-mathematicians. The χ2 statistic is a statis- tic, that is, it can be calculated from a data sample using actual numbers. The χ2 distribution is a theoretical construct. If a sufficiently large number of chi-square statistics are calculated, all from true random samples of the same population, then this population of χ2 statistics will, provably, fit a χ2 distribution. This is also true for other statistics: that is, if a sufficiently large number of log-likelihood statistics are calculated, all from true random samples of the same population, then this population of log-likelihood statistics will, provably, fit a χ2 distribution. Some texts call the statistic χ2 rather than χ2 to distinguish it more clearly from the distribution, but this practice is in the minority and is not adopted here. 5. See appendix 6. The model used was a sophisticated one incorporating evidence about type fre- quencies of verbs from the ANLT lexicon: see Briscoe and Carroll (1997) or Kor- honen, Correll, and McCarthy (2000) for details. 7. See Kilgarriff and Grefenstette (2003) and papers therein. The web is a vast re- source for many languages. See also Banko and Brill (2001) for the benefits of large data over sophisticated mathematics. \f274 A. Kilgarriff Appendix The average value of the error term is 0.5. We explain this as follows. If we do in fact have a random distribution, then by the definition of the χ2 distribution, the sum of the cells in the contingency table is 1: a⫹b⫹c⫹d⫽1 Each of these error terms is calculated as (O ⫺ E ⫺ 0.5) 2/E In our situation, there are very large datasets and the phenomenon of interest only accounts for a very small proportion of cases. The fre- quency of not word w is very high. Thus the expected values, E, for not word w to be used when calculating c and d for the contingency table are very high. As we divide by very large E, c and d are vanishingly small, so a⫹b⫹c⫹d⫽1 reduces to a⫹b⫽1 Since we have set the situation up symmetrically, a and b are the same size, so each will be, on average, 0.5. References Banko, Michele and Eric Brill 2001 Scaling to very very large corpora for natural language disambiguation. Proceedings of the 39th Annual Meeting of the Association for Computa- tional Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics. Bod, Rens 1995 Enriching linguistics with statistics: performance models of natural lan- guage. Ph.D. dissertation, University of Amsterdam. Brandstätter, E. 1999 Confidence intervals as an alternative to significance testing. Methods of Psychological Research Outline 4(2), 33⫺46. Brent, Michael R. 1993 From grammar to lexicon: unsupervised learning of lexical syntax. Com- putational Linguistics 19(2), 243⫺262. Briscoe, Ted and John Carroll 1997 Automatic extraction of subcategorization from corpora. Proceedings of the Fifth Conference on Applied Natural Language Processing, 356⫺363. \f Language is never, ever, ever, random 275 Carver, R. P. 1993 The case against statistical significance testing, revisited. Journal of Ex- perimental Education 61, 287⫺292. Church, Kenneth and Patrick Hanks 1990 Word association norms, mutual information and lexicography. Compu- tational Linguistics 16(1), 22⫺29. Dunning, Ted 1993 Accurate methods for the statistics of surprise and coincidence. Compu- tational Linguistics 19(1), 61⫺74. Gale, William and Geoffrey Sampson 1995 Good-Turing frequency estimation without tears. Journal of Quantitative Linguistics 2(3), Good, I. J. 1953 The population frequencies of species and the estimation of population parameters. Biometrika 40, 237⫺264. Grefenstette, Gregory and Julien Nioche 2000 Estimation of English and non-English language use on the www. In Proceedings of RIAO (Recherche d’Informations Assiste´ e par Ordinateur), 237⫺246. Hofland, Knud and Stig Johanson (Eds.) 1982 Word Frequencies in British and American English. Bergen: The Norwe- gian Computing Centre for the Humanities. Korhonen, Anna 2000 Using semantically motivated estimates to help subcategorization acquisition. Proceedings of the Joint Conference on Empirical Methods in NLP and Very Large Corpora, 216⫺223. Korhonen, Anna, Genevieve Gorrell, and Diana McCarthy 2000 Statistical filtering and subcategorization frame acquisition. Proceedings of the Joint Conference on Empirical Methods in NLP and Very Large Corpora, 199⫺206. LDOCE 1995 Longman Dictionary of Contemporary English, 3rd Edition. Ed. Della Summers. Harlow: Longman. Leech, Geoffrey and Roger Fallon 1992 Computer corpora — what do they tell us about culture? ICAME Journal 16, 29⫺50. Manning, Christopher and Hinrich Schütze 1999 Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press. Owen, Frank and Ronald Jones 1977 Statistics. Polytech Publishers. Pedersen, Ted 1996 Fishing for exactness. Proceedings of the Conference of the South-Central SAS Users Group, 188⫺200. Rayson, Paul and Roger Garside 2000 Comparing corpora using frequency profiling. Proceedings of the Work- shop on Comparing Corpora, 38th ACL, 1⫺6. Rayson, Paul, Geoffrey Leech, and Mary Hodges 1997 Social differentiation in the use of English vocabulary: some analysis of the conversational component of the British National Corpus. Interna- tional Journal of Corpus Linguistics 2(1), 133⫺152. Stubbs, Michael 1995 Collocations and semantic profiles: On the cause of the trouble with quantitative studies. Functions of Language 2(1), 23⫺55. \f\n"]}],"source":["# Viết code của bạn tại đây\n","print(text)"]},{"cell_type":"code","source":["doc = nlp(text)"],"metadata":{"id":"6lsH6oDYTvF7","executionInfo":{"status":"ok","timestamp":1684494106563,"user_tz":-420,"elapsed":29,"user":{"displayName":"Quân Tiến Quân","userId":"08559202489371626776"}}},"id":"6lsH6oDYTvF7","execution_count":5,"outputs":[]},{"cell_type":"code","source":["for token in doc:\n","  print(token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2fbuO_nUAh4","executionInfo":{"status":"ok","timestamp":1684494108636,"user_tz":-420,"elapsed":2102,"user":{"displayName":"Quân Tiến Quân","userId":"08559202489371626776"}},"outputId":"6aa7f1ae-0f80-4c80-b326-b84ff2e48ede"},"id":"e2fbuO_nUAh4","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n","symmetric\n",":\n","P\n","(\n","x|y\n",")\n","⫽\n","P\n","(\n","x|ÿ\n","y\n",")\n","entails\n","P\n","(\n","y|x\n",")\n","⫽\n","P\n","(\n","y|ÿ\n","x\n",")\n",".\n","Hereafter\n","I\n","use\n","‘\n","random\n","’\n","for\n","the\n","technical\n","meaning\n","and\n","‘\n","arbi-\n","trary\n","’\n","for\n","the\n","non\n","-\n","technical\n","one\n",".\n","Arbitrary\n","events\n","are\n","very\n","rarely\n","random\n",",\n","and\n","random\n","events\n","are\n","very\n","rarely\n","arbitrary\n",".\n","It\n","takes\n","considerable\n","ingenuity\n","and\n","sophisticated\n","mathe-\n","matics\n","to\n","produce\n","a\n","pseudo\n","-\n","random\n","sequence\n","algorithmically\n",",\n","and\n","true\n","randomness\n","is\n","not\n","possible\n","at\n","all\n",".\n","Events\n","happening\n","“\n","without\n","any\n","defi-\n","nite\n","plan\n",",\n","aim\n","or\n","pattern\n","”\n","are\n",",\n","by\n","definition\n",",\n","arbitrary\n",",\n","but\n","are\n","vanish-\n","ingly\n","unlikely\n","to\n","be\n","random\n",".\n","Outside\n","the\n","sub\n","-\n","atomic\n","realm\n",",\n","natural\n","events\n","are\n","very\n","rarely\n","random\n",".\n","Consider\n",",\n","for\n","example\n",",\n","cat\n","food\n","purchases\n","and\n","shoe\n","-\n","polish\n","purchases\n","within\n","the\n","space\n","of\n","all\n","UK\n","supermarket\n","-\n","shopping\n","events\n",":\n","does\n","the\n","fact\n","that\n","cat\n","food\n","was\n","bought\n","predict\n","(\n","positively\n","or\n","negatively\n",")\n","whether\n","shoe\n","polish\n","was\n","bought\n","in\n","the\n","same\n","shopping\n","trip\n","?\n","There\n","is\n","no\n","obvious\n","reason\n","why\n","it\n","should\n",",\n","and\n","we\n","can\n","happily\n","declare\n","the\n","relation\n","arbitrary\n",".\n","But\n","perhaps\n","either\n","cat\n","food\n","or\n","shoe\n","-\n","polish\n","are\n","more\n","(\n","or\n","less\n",")\n","often\n","bought\n","in\n","hot\n","(\n","or\n","cold\n",")\n","weather\n",",\n","or\n","on\n","Saturday\n","nights\n",",\n","or\n","Sunday\n","mornings\n",",\n","or\n","Monday\n","lunchtimes\n",",\n","or\n","by\n","richer\n","(\n","or\n","poorer\n",")\n","people\n",",\n","or\n","by\n","men\n","(\n","or\n","women\n",")\n",",\n","or\n","by\n","people\n","in\n","(\n","or\n","out\n","of\n",")\n","towns\n","…\n","There\n","is\n","an\n","unlimited\n","number\n","of\n","hypotheses\n","connecting\n","the\n","two\n","(\n","positively\n","or\n","negatively\n",")\n",";\n","if\n","just\n","one\n","of\n","these\n","has\n","any\n","validity\n",",\n","however\n","weak\n",",\n","then\n","the\n","null\n","hypothesis\n","is\n","false\n",".\n","At\n","this\n","point\n",",\n","you\n","may\n","question\n","why\n","the\n","null\n","hypothesis\n","is\n","ever\n","a\n","useful\n","construct\n",".\n","For\n","a\n","wide\n","range\n","of\n","tasks\n",",\n","although\n","H0\n","is\n","false\n",",\n","there\n","is\n","only\n","enough\n","evidence\n","to\n","establish\n","the\n","fact\n","if\n","there\n","is\n","a\n","strong\n","relation\n","between\n","the\n","two\n","phenomena\n",".\n","Thus\n",",\n","given\n","evidence\n","from\n","1,000\n","shopping\n","trips\n",",\n","it\n","is\n","un-\n","likely\n","we\n","shall\n","be\n","able\n","to\n","reject\n","H0\n","concerning\n","cat\n","food\n","and\n","shoe\n","-\n","polish\n",",\n","whereas\n","we\n","shall\n","be\n","able\n","to\n","reject\n","it\n","concerning\n","strawberry\n","-\n","buying\n","and\n","cream\n","-\n","buying\n",".\n","Given\n","further\n","evidence\n",",\n","perhaps\n","from\n","1,000,000\n","shopping\n","\f\n","266\n","A.\n","Kilgarriff\n","trips\n",",\n","we\n","shall\n","also\n","be\n","able\n","to\n","reject\n","the\n","null\n","hypothesis\n","regarding\n","nappy2-\n","buying\n","and\n","beer\n","-\n","sixpack\n","-\n","buying\n",".\n","(\n","The\n","correlation\n",",\n","the\n","most\n","newsworthy\n","product\n","of\n","large\n","-\n","scale\n","data\n","mining\n","by\n","supermarkets\n",",\n","was\n","widely\n","reported\n","in\n","the\n","British\n","media\n",".\n",")\n","But\n","still\n","not\n","for\n","catf\n","ood\n","and\n","shoe\n","-\n","polish\n",".\n","But\n",",\n","given\n","1,000,000,000\n","events\n",",\n","we\n","shall\n","in\n","all\n","likelihood\n","also\n","be\n","able\n","to\n","reject\n","it\n","for\n","cat\n","food\n","and\n","shoe\n","-\n","polish\n",".\n","Whether\n","or\n","not\n","we\n","can\n","reject\n","the\n","null\n","hypothesis\n","(\n","with\n","eg\n",".\n","95\n","%\n","confi-\n","dence\n",")\n","is\n","a\n","function\n","of\n","sample\n","size\n","and\n","level\n","of\n","correlation\n",".\n","Where\n","sample\n","size\n","is\n","held\n","constant\n","(\n","and\n","is\n","not\n","enormous\n",")\n",",\n","whether\n","or\n","not\n","we\n","can\n","reject\n","H0\n","can\n","be\n","seen\n","as\n","a\n","way\n","of\n","providing\n","statistical\n","support\n","for\n","distinguish-\n","ing\n","the\n","arbitrary\n","and\n","the\n","motivated\n",".\n","This\n","is\n","a\n","role\n","that\n","hypothesis\n","testing\n","plays\n","across\n","the\n","social\n","sciences\n",".\n","However\n","where\n","the\n","sample\n","size\n","varies\n","by\n","an\n","order\n","of\n","magnitude\n",",\n","or\n","where\n","it\n","is\n","enormous\n",",\n","it\n","is\n","wrong\n","to\n","identify\n","the\n","accept\n","-\n","H0\n","/\n","reject\n","-\n","H0\n","distinction\n","with\n","the\n","arbitrary\n","/\n","motivated\n","one\n",".\n","The\n","uneasy\n","relationship\n","between\n","hypothesis\n","-\n","testing\n",",\n","and\n","quantity\n","of\n","data\n",",\n","is\n","familiar\n","to\n","statisticians\n","though\n","frequently\n","overlooked\n","or\n","mis-\n","understood\n","by\n","users\n","of\n","statistics\n","(\n","Carver\n","1993\n",",\n","Stubbs\n","1995\n",",\n","Brandstätter\n","1999\n",")\n",".\n","One\n","statistics\n","textbook\n","warns\n","thus\n",":\n","None\n","of\n","the\n","null\n","hypotheses\n","we\n","have\n","considered\n","with\n","respect\n","to\n","good-\n","ness\n","of\n","fit\n","can\n","be\n","exactly\n","true\n",",\n","so\n","if\n","we\n","increase\n","the\n","sample\n","size\n","(\n","and\n","hence\n","the\n","value\n","of\n","χ2\n",")\n","we\n","would\n","ultimately\n","reach\n","the\n","point\n","when\n","all\n","null\n","hypotheses\n","would\n","be\n","rejected\n",".\n","All\n","that\n","the\n","χ2\n","test\n","can\n","tell\n","us\n",",\n","then\n",",\n","is\n","that\n","the\n","sample\n","size\n","is\n","too\n","small\n","to\n","reject\n","the\n","null\n","hypothesis\n","!\n","(\n","Owen\n","and\n","Jones\n",",\n","1977\n",",\n","p\n","359\n",")\n",".\n","The\n","issue\n","is\n","particularly\n","salient\n","for\n","empirical\n","linguistics\n","because\n",",\n","firstly\n",",\n","we\n","have\n","access\n","to\n","extremely\n","large\n","sample\n","sizes\n",",\n","and\n","secondly\n",",\n","the\n","distri-\n","bution\n","of\n","many\n","language\n","phenomena\n","is\n","Zipfian\n",".\n","The\n","has\n","6,000,000\n","oc-\n","currences\n","in\n","the\n","BNC\n","whereas\n","cat\n","food\n","(\n","spelled\n","as\n","one\n","word\n","or\n","two\n",")\n","has\n","66\n",".\n","For\n","a\n","vast\n","number\n","of\n","third\n","phenomena\n","X\n",",\n","the\n","null\n","hypothesis\n","that\n","the\n","and\n","X\n","are\n","uncorrelated\n","will\n","be\n","rejected\n",",\n","whereas\n","the\n","null\n","hypothesis\n","that\n","cat\n","food\n","and\n","X\n","are\n","uncorrelated\n","will\n","not\n",".\n","It\n","would\n","be\n","wrong\n","to\n","draw\n","inferences\n","about\n","what\n","was\n","arbitrary\n",",\n","what\n","motivated\n",".\n","3\n",".\n","Objections\n","to\n","Maximum\n","Likelihood\n","Estimates\n","(\n","MLEs\n",")\n","Church\n","and\n","Hanks\n","(\n","1990\n",")\n","inaugurated\n","the\n","research\n","area\n","of\n","lexical\n","statis-\n","tics\n","with\n","their\n","presentation\n","of\n","Mutual\n","Information\n","(\n","I\n",")\n",",\n","a\n","measure\n","of\n","how\n","closely\n","associated\n","two\n","phenomena\n","are\n",".\n","It\n","can\n","be\n","applied\n","to\n","finding\n","words\n","which\n","occur\n","together\n","to\n","a\n","noteworthy\n","degree\n",",\n","or\n","to\n","finding\n","words\n","which\n","are\n","particularly\n","associated\n","with\n","one\n","corpus\n","as\n","against\n","another\n",",\n","or\n","for\n","various\n","other\n","purposes.3\n","They\n","define\n","the\n","mutual\n","information\n","between\n","two\n","words\n","x\n","and\n","y\n","as\n","\f \n","Language\n","is\n","never\n",",\n","ever\n",",\n","ever\n",",\n","random\n","267\n","I\n","(\n","x\n",";\n","y\n",")\n","⫽\n","log\n","2\n","冉\n","p\n","(\n","xandy\n",")\n","p\n","(\n","x\n",")\n","·\n","p(y\n",")\n","冊\n","and\n","then\n","estimate\n","probabilities\n","directly\n","from\n","frequencies\n",",\n","that\n","is\n","using\n","the\n","‘\n","Maximum\n","Likelihood\n","Estimate\n","’\n","(\n","MLE\n",")\n","of\n","f\n","(\n","x)/N\n","for\n","p\n","(\n","x\n",")\n",",\n","f\n","(\n","y)/N\n","for\n","p\n","(\n","y\n",")\n",",\n","f\n","(\n","x\n","-\n","and\n","-\n","y)/N\n","for\n","p\n","(\n","x\n","-\n","and\n","-\n","y\n",")\n",",\n","thereby\n","giving\n","I\n","(\n","x\n",";\n","y\n",")\n","⫽\n","log\n","2\n","冉\n","N\n","·\n","f\n","(\n","x\n","⫺\n","and\n","⫺\n","y\n",")\n","f\n","(\n","x\n",")\n","·\n","f\n","(\n","y\n",")\n","冊\n","Dunning\n","(\n","1993\n",")\n","presents\n","a\n","critique\n","of\n","the\n","use\n","of\n","Mutual\n","Information\n","in\n","empirical\n","linguistics\n",".\n","His\n","objection\n","has\n","been\n","confused\n","with\n","the\n","critique\n","of\n","hypothesis\n","-\n","testing\n","I\n","make\n","here\n","so\n","I\n","mention\n","his\n","work\n","in\n","order\n","to\n","clarify\n","that\n","the\n","two\n","objections\n",",\n","while\n","both\n","valid\n",",\n","are\n","different\n","in\n","nature\n","and\n","independent\n",".\n","Dunning\n","demonstrates\n","how\n","MLEs\n","fare\n","poorly\n","when\n","estimating\n","the\n","probabilities\n","of\n","rare\n","events\n",".\n","The\n","problem\n","is\n","essentially\n","this\n",":\n","if\n","a\n","word\n","(\n","or\n","bigram\n",",\n","or\n","trigram\n",",\n","or\n","character\n","-\n","sequence\n","etc\n",".\n",")\n","occurs\n","just\n","once\n","or\n","twice\n","in\n","a\n","corpus\n","of\n","N\n","words\n","(\n","bigrams\n",",\n","etc\n",".\n",")\n",",\n","then\n","the\n","simplest\n","way\n","to\n","estimate\n","the\n","probability\n","is\n","the\n","NILE\n",",\n","which\n","gives\n","1\n","/\n","N\n","or\n","2\n","/\n","N.\n","However\n","this\n","does\n","not\n","factor\n","in\n","the\n","arbitrariness\n","of\n","the\n","word\n","occurring\n","at\n","all\n","in\n","the\n","corpus\n",":\n","in\n","a\n","corpus\n","ten\n","times\n","the\n","size\n",",\n","there\n","would\n","be\n","roughly\n","ten\n","times\n","the\n","number\n","of\n","singletons\n","and\n","doubletons\n","in\n","the\n","corpus\n",",\n","most\n","of\n","which\n","would\n","not\n","have\n","occurred\n","at\n","all\n","in\n","the\n","original\n","corpus\n",".\n","Thus\n","some\n","of\n","the\n","prob-\n","ability\n","mass\n","contributing\n","to\n","the\n","1\n","/\n","N\n","or\n","2\n","/\n","N\n","MLEs\n","should\n","have\n","been\n","put\n","aside\n","for\n","the\n","words\n","(\n","bigrams\n","etc\n",".\n",")\n","which\n","did\n","not\n","occur\n","at\n","all\n","in\n","this\n","particular\n","corpus\n",".\n","Viewed\n","another\n","way\n",",\n","the\n","1\n","/\n","N\n","and\n","2\n","/\n","N\n","should\n","be\n","dis-\n","counted\n","to\n","allow\n","for\n","the\n","fact\n","that\n","one\n","or\n","two\n","occurrences\n","are\n","very\n","low\n","bases\n","of\n","evidence\n","on\n","which\n","to\n","assert\n","probabilities\n",".\n","There\n","are\n","various\n","ways\n","in\n","which\n","the\n","discounting\n","can\n","be\n","done\n",",\n","for\n","example\n","the\n","Good\n","-\n","Turing\n","method\n","(\n","Good\n","1953\n",")\n",",\n","usefully\n","applied\n","to\n","em-\n","pirical\n","linguistics\n","in\n","Gale\n","and\n","Sampson\n","(\n","1995\n",")\n",",\n","Bod\n","(\n","1995\n",")\n",".\n","Dunning\n","presents\n","and\n","advocates\n","the\n","use\n","of\n","the\n","log\n","-\n","likelihood\n","statistic\n",",\n","which\n",",\n","like\n","the\n","χ2\n","statistic\n",",\n","is\n","χ2\n","-\n","distributed,4\n","but\n","more\n","accurately\n","estimates\n","probabil-\n","ities\n","where\n","counts\n","are\n","low\n",".\n","The\n","log\n","-\n","likelihood\n","statistic\n","still\n","only\n","estimates\n","probabilities\n",":\n","since\n","Dunning\n","’s\n","work\n",",\n","Pedersen\n","(\n","1996\n",")\n","has\n","shown\n","how\n","Fisher\n","’s\n","Exact\n","Method\n","can\n","be\n","applied\n","to\n","the\n","problem\n",",\n","to\n","identify\n","the\n","exact\n","probability\n","of\n","a\n","word\n","(\n","bigram\n","etc\n",".\n",")\n","rather\n","than\n","estimating\n","it\n","at\n","all\n",".\n","Thus\n","Dunning\n","’s\n","objection\n","to\n","Mutual\n","Information\n","is\n","that\n","it\n","fails\n","to\n","accurately\n","represent\n","probabilities\n","when\n","counts\n","are\n","low\n","(\n","where\n","‘\n","low\n","’\n","is\n","generally\n","taken\n","as\n","less\n","than\n","five\n",")\n",".\n","If\n","the\n","probabilities\n","can\n","be\n","accurately\n","represented\n",",\n","Dunning\n","’s\n","anxieties\n","will\n","be\n","set\n","at\n","ease\n",".\n","\f\n","268\n","A.\n","Kilgarriff\n","The\n","critique\n","in\n","this\n","paper\n","does\n","not\n","concern\n","whether\n","probabilities\n","are\n","accurately\n","calculated\n",".\n","Rather\n",",\n","the\n","objection\n","is\n","that\n","the\n","probability\n","model\n",",\n","with\n","its\n","assumptions\n","of\n","randomness\n",",\n","is\n","inappropriate\n",",\n","particularly\n","where\n","counts\n","are\n","high\n","(\n","eg\n",",\n","thousands\n","or\n","more\n",")\n",".\n","Where\n","the\n","task\n","is\n","to\n","determine\n","whether\n","there\n","is\n","an\n","interesting\n","associa-\n","tion\n","between\n","two\n","rare\n","events\n",",\n","Dunning\n","’s\n","concern\n","must\n","be\n","heeded\n",".\n","Where\n","it\n","is\n","to\n","determine\n","whether\n","there\n","is\n","an\n","interesting\n","association\n","between\n","high\n","-\n","frequency\n","events\n",",\n","the\n","concerns\n","of\n","this\n","paper\n","must\n","be\n",".\n","4\n",".\n","Experiment\n","Given\n","enough\n","data\n",",\n","H0\n","is\n","almost\n","always\n","rejected\n","however\n","arbitrary\n","the\n","data\n",",\n","as\n","the\n","author\n","discovered\n","when\n","grappling\n","with\n","the\n","following\n","data\n",".\n","Two\n","corpora\n","were\n","set\n","up\n","to\n","be\n","indisputably\n","of\n","the\n","same\n","language\n","type\n",",\n","with\n","only\n","arbitrary\n","differences\n","between\n","them\n",":\n","each\n","was\n","a\n","random\n","subset\n","of\n","the\n","written\n","part\n","of\n","the\n","British\n","National\n","Corpus\n","(\n","BNC\n",")\n",".\n","The\n","sampling\n","was\n","as\n","follows\n",":\n","all\n","texts\n","shorter\n","than\n","20,000\n","words\n","were\n","excluded\n",".\n","This\n","left\n","820\n","texts\n",".\n","Half\n","the\n","texts\n","were\n","then\n","randomly\n","assigned\n","to\n","each\n","of\n","two\n","corpora\n",".\n","The\n","null\n","hypotheses\n","were\n","(\n","1\n",")\n","that\n","the\n","two\n","subcorpora\n",",\n","viewed\n","as\n","col-\n","lections\n","of\n","words\n","rather\n","than\n","documents\n",",\n","were\n","random\n","samples\n","drawn\n","from\n","the\n","same\n","population\n",";\n","and\n","consequently\n",",\n","(\n","2\n",")\n","that\n","the\n","deviation\n","in\n","frequency\n","of\n","occurrence\n","for\n","each\n","individual\n","word\n","between\n","the\n","two\n","sub-\n","corpora\n","was\n","explicable\n","as\n","random\n","fluctuation\n",".\n","The\n","HO\n","were\n","tested\n","using\n","the\n","χ2\n","-\n","test\n",":\n","is\n","χ2\n","Σ(|O\n","⫺\n","E\n","|\n","⫺\n","0.5\n",")\n","2\n","/E\n","greater\n","than\n","the\n","critical\n","value\n","?\n","The\n","sum\n","is\n","over\n","the\n","four\n","cells\n","of\n","the\n","contingency\n","table\n","Corpus\n","1\n","Corpus\n","2\n","word\n","w\n","a\n","b\n","not\n","word\n","w\n","c\n","d\n","If\n","we\n","randomly\n","assign\n","words\n","(\n","as\n","opposed\n","to\n","documents\n",")\n","to\n","the\n","one\n","corpus\n","or\n","the\n","other\n",",\n","then\n","we\n","have\n","a\n","straightforward\n","random\n","distribution\n",",\n","with\n","the\n","value\n","of\n","the\n","χ2\n","-\n","statistic\n","equal\n","to\n","or\n","greater\n","than\n","the\n","99.5\n","%\n","confidence\n","threshold\n","of\n","7.88\n","for\n","just\n","0.5\n","%\n","of\n","words\n",".\n","The\n","average\n","value\n","of\n","the\n","error\n","term\n",",\n","\f \n","Language\n","is\n","never\n",",\n","ever\n",",\n","ever\n",",\n","random\n","269\n","(\n","|O\n","⫺\n","E|\n","⫺\n","0.5\n",")\n","2\n","/E\n","is\n","then\n","0.5.5\n","The\n","hypothesis\n","can\n",",\n","therefore\n",",\n","be\n","couched\n","as\n",":\n","are\n","the\n","error\n","terms\n","systematically\n","greater\n","than\n","0.5\n","?\n","If\n","they\n","are\n",",\n","we\n","should\n","be\n","wary\n","of\n","attributing\n","high\n","error\n","terms\n","to\n","significant\n","differences\n","between\n","text\n","types\n",",\n","since\n","we\n","also\n","obtain\n","many\n","high\n","error\n","terms\n","where\n","there\n","are\n","no\n","significant\n","differences\n","between\n","text\n","types\n",".\n","Frequency\n","lists\n","for\n","word\n","-\n","POS\n","pairs\n","for\n","each\n","subcorpus\n","were\n","gener-\n","ated\n",".\n","For\n","each\n","word\n","occurring\n","in\n","either\n","subcorpus\n",",\n","the\n","error\n","term\n","which\n","would\n","have\n","contributed\n","to\n","a\n","χ2\n","calculation\n","was\n","determined\n",".\n","As\n","Table\n","4\n","shows\n",",\n","average\n","values\n","for\n","the\n","error\n","term\n","are\n","far\n","greater\n","than\n","0.5\n",",\n","and\n","tend\n","to\n","increase\n","as\n","word\n","frequency\n","increases\n",".\n","As\n","the\n","averages\n","indicate\n",",\n","the\n","error\n","term\n","is\n","very\n","often\n","greater\n","than\n","0.5\n","*\n","7.88\n","⫽\n","3.94\n",",\n","the\n","relevant\n","critical\n","value\n","of\n","the\n","chi\n","-\n","square\n","statistic\n",".\n","For\n","very\n","many\n","words\n",",\n","including\n","most\n","common\n","words\n",",\n","the\n","null\n","hypothesis\n","is\n","resoundingly\n","defeated\n","(\n","as\n","is\n","the\n","null\n","hypthesis\n","regarding\n","the\n","two\n","subc-\n","orpora\n","as\n","wholes\n",")\n",".\n","There\n","is\n","no\n","a\n","priori\n","reason\n","to\n","expect\n","words\n","to\n","behave\n","as\n","if\n","they\n","had\n","been\n","selected\n","at\n","random\n",",\n","and\n","indeed\n","they\n","do\n","not\n",".\n","It\n","is\n","in\n","the\n","nature\n","of\n","Table\n","1\n",".\n","Comparing\n","two\n","same\n","-\n","genre\n","corpora\n","using\n","χ2\n","Class\n","First\n","item\n","in\n","class\n","Mean\n","error\n","term\n","(\n","Words\n","in\n","freq\n",".\n","order\n",")\n","for\n","items\n","in\n","class\n","word\n","POS\n","First\n","10\n","items\n","the\n","DET\n","18.76\n","Next\n","10\n","items\n","for\n","PRP\n","17.45\n","Next\n","20\n","items\n","not\n","XX\n","14.39\n","Next\n","40\n","items\n","have\n","VHB\n","10.71\n","Next\n","80\n","items\n","also\n","AVO\n","7.03\n","Next\n","160\n","items\n","know\n","VVI\n","6.40\n","Next\n","320\n","items\n","six\n","CRD\n","5.30\n","Next\n","640\n","items\n","finally\n","AV0\n","6.71\n","Next\n","1280\n","items\n","plants\n","NN2\n","6.05\n","Next\n","2560\n","items\n","pocket\n","NN1\n","5.82\n","Next\n","5120\n","items\n","represent\n","VVB\n","4.53\n","Next\n","10240\n","items\n","peking\n","NP0\n","3.07\n","Next\n","20480\n","items\n","fondly\n","AV0\n","1.87\n","Next\n","40960\n","items\n","chandelier\n","NN1\n","1.15\n","Table\n","note\n",".\n","Mean\n","error\n","term\n","is\n","far\n","greater\n","than\n","0.5\n",",\n","and\n","increases\n","with\n","frequency\n",".\n","POS\n","tags\n","are\n","drawn\n","from\n","the\n","CLAWS-5\n","tagset\n","as\n","used\n","in\n","the\n","BNC\n","(\n","see\n","http:/info.ox\n",".\n","ac.uk/bnc\n",")\n","\f\n","270\n","A.\n","Kilgarriff\n","language\n","that\n","any\n","two\n","collections\n","of\n","texts\n",",\n","covering\n","a\n","wide\n","range\n","of\n","registers\n","(\n","and\n","comprising\n",",\n","say\n",",\n","less\n","than\n","a\n","thousand\n","samples\n","of\n","over\n","a\n","thousand\n","words\n","each\n",")\n","will\n","show\n","such\n","differences\n",".\n","While\n","it\n","might\n","seem\n","plausible\n","that\n","oddities\n","would\n","in\n","some\n","way\n","balance\n","out\n","to\n","give\n","a\n","popula-\n","tion\n","that\n","was\n","indistinguishable\n","from\n","one\n","where\n","the\n","individual\n","words\n","(\n","as\n","opposed\n","to\n","the\n","texts\n",")\n","had\n","been\n","randomly\n","selected\n",",\n","this\n","turns\n","out\n","not\n","to\n","be\n","the\n","case\n",".\n","The\n","key\n","word\n","in\n","the\n","last\n","paragraph\n","is\n","‘\n","indistinguishable\n","’\n",".\n","In\n","hypothesis\n","testing\n",",\n","the\n","objective\n","is\n","generally\n","to\n","see\n","if\n","the\n","population\n","can\n","be\n","distin-\n","guished\n","from\n","one\n","that\n","has\n","been\n","randomly\n","generated\n","⫺\n","or\n",",\n","in\n","our\n","case\n",",\n","to\n","see\n","if\n","the\n","two\n","populations\n","are\n","distinguishable\n","from\n","two\n","populations\n","which\n","have\n","been\n","randomly\n","generated\n","on\n","the\n","basis\n","of\n","the\n","frequencies\n","in\n","the\n","joint\n","corpus\n",".\n","Since\n","words\n","in\n","a\n","text\n","are\n","not\n","random\n",",\n","we\n","know\n","that\n","our\n","corpora\n","are\n","not\n","randomly\n","generated\n",",\n","and\n","the\n","hypothesis\n","test\n","con-\n","firms\n","the\n","fact\n",".\n","5\n",".\n","Re\n","-\n","analysis\n","of\n","previous\n","work\n","5.1\n",".\n","Brown\n","and\n","LOB\n","Hofland\n","and\n","Johansson\n","(\n","1982\n",")\n","wanted\n","to\n","find\n","words\n","which\n","were\n","signifi-\n","cantly\n","different\n","in\n","their\n","frequencies\n","between\n","British\n","and\n","American\n","Eng-\n","lish\n",",\n","as\n","represented\n","in\n","the\n","Brown\n","corpus\n","for\n","American\n","English\n","and\n","LOB\n","corpus\n","for\n","British\n",".\n","For\n","each\n","word\n",",\n","they\n","tested\n","the\n","null\n","hypothesis\n","that\n","the\n","difference\n","in\n","frequency\n","between\n","the\n","two\n","corpora\n","could\n","be\n","explained\n","as\n","random\n","variation\n",",\n","with\n","the\n","samples\n","being\n","random\n","samples\n","from\n","the\n","same\n","source\n",",\n","and\n","in\n","their\n","frequency\n","lists\n",",\n","they\n","mark\n","words\n","where\n","the\n","null\n","hypothesis\n","was\n","defeated\n","(\n","at\n","a\n","95\n",",\n","99\n","or\n","99.9\n","%\n","confidence\n","level\n",")\n",".\n","Looking\n","at\n","these\n","lists\n","suggests\n","that\n","virtually\n","all\n","common\n","words\n","are\n","markedly\n","different\n","in\n","their\n","levels\n","of\n","use\n","between\n","the\n","US\n","and\n","the\n","UK\n",":\n","they\n","are\n","all\n","marked\n","as\n","such\n",".\n","By\n","contrast\n",",\n","most\n","of\n","the\n","rarer\n","marked\n","words\n","are\n","words\n","we\n","know\n","to\n","be\n","American\n","or\n","British\n",",\n","or\n","to\n","refer\n","to\n","items\n","that\n","are\n","more\n","common\n","or\n","more\n","salient\n","in\n","the\n","US\n","or\n","the\n","UK\n",".\n","As\n","the\n","argument\n","of\n","the\n","previous\n","section\n","explains\n",",\n","most\n","of\n","the\n","marked\n","high\n","-\n","frequency\n","words\n","are\n","marked\n","simply\n","as\n","a\n","consequence\n","of\n","the\n","essen-\n","tially\n","non\n","-\n","random\n","nature\n","of\n","language\n",".\n","It\n","would\n","not\n","be\n","surprising\n","for\n","a\n","high\n","-\n","frequency\n","word\n","marked\n","as\n","British\n","English\n","in\n","these\n","lists\n","to\n","be\n","marked\n","as\n","American\n","English\n","in\n","a\n","repeat\n","of\n","the\n","experiment\n","using\n","new\n","data\n",".\n","Similar\n","strategies\n","are\n","used\n","by\n",",\n","and\n","a\n","similar\n","critique\n","is\n","applicable\n","to\n",",\n","Leech\n","and\n","Fallon\n","(\n","1992\n",")\n","(\n","again\n",",\n","for\n","comparing\n","LOB\n","and\n","Brown\n",")\n",",\n","Ray-\n","\f \n","Language\n","is\n","never\n",",\n","ever\n",",\n","ever\n",",\n","random\n","271\n","son\n",",\n","Leech\n",",\n","and\n","Hodges\n","(\n","1997\n",")\n","for\n","comparing\n","the\n","conversation\n","of\n","dif-\n","ferent\n","social\n","groups\n",",\n","and\n","Rayson\n","and\n","Garside\n","(\n","2000\n",")\n","for\n","contrasting\n","the\n","language\n","of\n","a\n","specialist\n","genre\n","with\n","‘\n","general\n","language\n","’\n",",\n","as\n","represented\n","by\n","the\n","British\n","National\n","Corpus\n",".\n","5.2\n","Subcategorization\n","frame\n","(\n","SCF\n",")\n","learning\n","Hypothesis\n","-\n","testing\n","has\n","been\n","used\n","in\n","a\n","number\n","of\n","papers\n","concerning\n","the\n","automatic\n","acquisition\n","of\n","subcategorization\n","frames\n","(\n","SCFs\n",")\n","for\n","verbs\n","from\n","corpora\n",".\n","The\n","problem\n","is\n","this\n",".\n","Dictionaries\n",",\n","even\n","where\n","they\n","do\n","present\n","explicit\n","and\n","accurate\n","SCFs\n","for\n","verbs\n",",\n","are\n","not\n","complete\n",":\n","they\n","do\n","not\n","pre-\n","sent\n","all\n","the\n","frames\n","for\n","each\n","verb\n",".\n","This\n","gives\n","rise\n","to\n","many\n","parsing\n","errors\n",".\n","Researchers\n","including\n","Brent\n","(\n","1993\n",")\n",",\n","Briscoe\n","and\n","Carroll\n","(\n","1997\n",")\n","and\n","Kor-\n","honen\n","(\n","2000\n",")\n","have\n","developed\n","methods\n","for\n","SCF\n","acquisition\n",".\n","However\n",",\n","their\n","methods\n","are\n","inevitably\n","noisy\n",",\n","suffering\n",",\n","for\n","example\n",",\n","from\n","just\n","those\n","parser\n","errors\n","that\n","the\n","whole\n","process\n","is\n","designed\n","to\n","address\n",",\n","and\n","they\n","do\n","not\n","wish\n","to\n","accept\n","any\n","SCF\n","for\n","which\n","there\n","is\n","any\n","evidence\n","as\n","a\n","true\n","SCF\n","for\n","the\n","verb\n",".\n","They\n","wish\n","to\n","filter\n","out\n","those\n","SCFs\n","where\n","the\n","evidence\n","is\n","not\n","strong\n","enough\n",".\n","Brent\n","and\n","Briscoe\n","and\n","Carroll\n","used\n","hypothesis\n","testing\n","to\n","this\n","end\n",".\n","However\n",",\n","problems\n","are\n","noted\n",":\n","Further\n","evaluation\n","of\n","the\n","results\n","...\n","reveals\n","that\n","the\n","filtering\n","phase\n","is\n","the\n","weak\n","link\n","in\n","the\n","system\n","…\n","The\n","performance\n","of\n","the\n","filter\n","for\n","classes\n","with\n","less\n","than\n","10\n","exemplars\n","is\n","around\n","chance\n",",\n","and\n","a\n","simple\n","heuristic\n","of\n","accepting\n","all\n","classes\n","with\n","more\n","then\n","10\n","exemplars\n","would\n","have\n","pro-\n","duced\n","broadly\n","similar\n","results\n","for\n","these\n","verbs\n","(\n","Briscoe\n","and\n","Carroll\n","1997\n",":\n","360⫺36\n",")\n",".\n","Korhonen\n",",\n","Correll\n",",\n","and\n","McCarthy\n","(\n","2000\n",")\n","explore\n","the\n","issue\n","in\n","detail\n",".\n","Using\n","Briscoe\n","and\n","Carroll\n","’s\n","SCF\n","acquisition\n","system\n",",\n","they\n","explore\n","the\n","impact\n","of\n","four\n","different\n","strategies\n","for\n","filtering\n","out\n","noise\n",":\n","Baseline\n","No\n","filter\n","BHT\n","binomial\n","hypothesis\n","test\n",":\n","reject\n","the\n","SCF\n","if\n","Ho\n","is\n","not\n","defeated6\n","LLR\n","hypothesis\n","test\n","using\n","log\n","-\n","likelihood\n","ratio\n",":\n","reject\n","the\n","SCF\n","if\n","Ho\n","is\n","not\n","defeated\n","MLE\n","threshold\n","based\n","on\n","the\n","relative\n","frequency\n","(\n","which\n","is\n","also\n","the\n","maximum\n","likelihood\n","estimate\n","(\n","MLE\n",")\n","of\n","the\n","probability\n",")\n","of\n","the\n","verb\n","occurring\n","in\n","the\n","SCF\n","given\n","the\n","verb\n",",\n","with\n","the\n","thresh-\n","old\n","determined\n","empirically\n","\f\n","272\n","A.\n","Kilgarriff\n","They\n","observe\n","MLE\n","thresholding\n","produced\n","better\n","results\n","than\n","the\n","two\n","statistical\n","tests\n","used\n",".\n","Precision\n","improved\n","considerably\n",",\n","showing\n","that\n","the\n","classes\n","occur-\n","ring\n","in\n","the\n","data\n","with\n","the\n","highest\n","frequency\n","are\n","often\n","correct\n","…\n","MLE\n","is\n","not\n","adept\n","at\n","finding\n","low\n","frequency\n","SCFs\n","…\n","(\n","Korhonen\n",",\n","Correll\n",",\n","and\n","McCarthy\n","2000\n",":\n","202\n",")\n","This\n","concurs\n","with\n","the\n","theoretical\n","argument\n","above\n",".\n","Hypothesis\n","tests\n","are\n","inappropriate\n","for\n","the\n","task\n",",\n","because\n","the\n","relations\n","between\n","verb\n","and\n","SCF\n","will\n","never\n","be\n","random\n","and\n","the\n","hypothesis\n","test\n","will\n","merely\n","reject\n","the\n","null\n","hypothesis\n","wherever\n","there\n","is\n","enough\n","data\n",",\n","in\n","a\n","manner\n","not\n","closely\n","corre-\n","lated\n","with\n","whether\n","the\n","SCF\n","-\n","verb\n","link\n","is\n","motivated\n",".\n","Where\n","there\n","is\n","enough\n","data\n",",\n","then\n","the\n","relationship\n","between\n","verb\n","and\n","SCF\n","is\n","easy\n","to\n","see\n","so\n","even\n","a\n","simple\n","threshold\n","method\n","will\n","identify\n","the\n","verb\n","’s\n","SCFs\n",".\n","Where\n","data\n","is\n","very\n","sparse\n",",\n","no\n","method\n","works\n","well\n",".\n","Korhonen\n","(\n","2000\n",")\n","extends\n","this\n","line\n","of\n","work\n",",\n","exploring\n","thresholding\n","methods\n","where\n","a\n","more\n","accurate\n","estimate\n","of\n","the\n","probability\n","is\n","obtained\n","by\n","using\n","data\n","from\n","semantically\n","similar\n","but\n","higher\n","frequency\n","verbs\n",".\n","She\n","achieves\n","modest\n","improvements\n","over\n","the\n","baseline\n","which\n","uses\n","Korhonen\n",",\n","Correll\n","and\n","McCarthy\n","’s\n","MLE\n",",\n","particularly\n","when\n","combining\n","the\n","fre-\n","quencies\n","of\n","the\n","target\n","verb\n","and\n","its\n","semantic\n","neighbour\n","using\n","a\n","linear\n","method\n","based\n","on\n","the\n","quantity\n","of\n","evidence\n","available\n","for\n","each\n",".\n","The\n","problem\n","is\n","not\n","one\n","of\n","distinguishing\n","random\n","and\n","non\n","-\n","random\n","relationships\n",",\n","but\n","of\n","sparseness\n","of\n","data\n",".\n","Where\n","the\n","data\n","is\n","not\n","sparse\n",",\n","the\n","difference\n","between\n","arbitrary\n","and\n","motivated\n","connections\n","is\n","evident\n","in\n","greatly\n","differing\n","relative\n","frequencies\n",".\n","This\n","makes\n","the\n","moral\n","of\n","the\n","story\n","plain\n",".\n","Data\n","is\n","abundant\n",".\n","A\n","modest\n","-\n","frequency\n","verb\n","like\n","devastate\n","occurs\n","(\n","Google\n","tells\n","us\n",")\n","in\n","well\n","over\n","a\n","million\n","web\n","pages\n",".\n","With\n","just\n","1\n","%\n","of\n","them\n",",\n","devastate\n","becomes\n","one\n","of\n","the\n","verbs\n","for\n","which\n","we\n","have\n","plenty\n","of\n","data\n",",\n","and\n","crude\n","thresholding\n","methods\n","will\n","distinguish\n","associated\n","SCFs\n","from\n","noise\n",".\n","It\n","is\n","possible\n","that\n","parsing\n","errors\n","are\n","systematic\n","and\n","thus\n","that\n","the\n","same\n","errors\n","occur\n","very\n","often\n","in\n","very\n","large\n","corpora\n","although\n","our\n","experi-\n","ence\n","from\n","looking\n","at\n","large\n","corpora\n","in\n","the\n","Word\n","Sketch\n","Engine\n","(\n","Kilgarriff\n","et\n","al\n","2004\n",")\n","suggests\n","not\n",".\n","Harvesting\n","the\n","web\n","(\n","or\n","other\n","huge\n","corpora\n",")\n","is\n","the\n","way\n","to\n","build\n","an\n","accurate\n","SCF\n","lexicon.7\n","6\n",".\n","Conclusion\n","Language\n","is\n","non\n","-\n","random\n","and\n","hence\n",",\n","when\n","we\n","look\n","at\n","linguistic\n","phenom-\n","ena\n","in\n","corpora\n",",\n","the\n","null\n","hypothesis\n","will\n","never\n","be\n","true\n",".\n","Moreover\n",",\n","where\n","there\n","is\n","enough\n","data\n",",\n","we\n","shall\n","(\n","almost\n",")\n","always\n","be\n","able\n","to\n","establish\n","that\n","it\n","is\n","not\n","true\n",".\n","In\n","corpus\n","studies\n",",\n","we\n","frequently\n","do\n","have\n","enough\n","data\n",",\n","so\n","\f \n","Language\n","is\n","never\n",",\n","ever\n",",\n","ever\n",",\n","random\n","273\n","the\n","fact\n","that\n","a\n","relation\n","between\n","two\n","phenomena\n","is\n","demonstrably\n","non-\n","random\n",",\n","does\n","not\n","support\n","the\n","inference\n","that\n","it\n","is\n","not\n","arbitrary\n",".\n","Hypoth-\n","esis\n","testing\n","is\n","rarely\n","useful\n","for\n","distinguishing\n","associated\n","from\n","non\n","-\n","associ-\n","ated\n","pairs\n","of\n","phenomena\n","in\n","large\n","corpora\n",".\n","Where\n","used\n",",\n","it\n","has\n","often\n","led\n","to\n","unhelpful\n","or\n","misleading\n","results\n",".\n","Hypothesis\n","testing\n","has\n","been\n","used\n","to\n","reach\n","conclusions\n",",\n","where\n","the\n","diffi-\n","culty\n","in\n","reaching\n","a\n","conclusion\n","is\n","caused\n","by\n","sparsity\n","of\n","data\n",".\n","But\n","language\n","data\n",",\n","in\n","this\n","age\n","of\n","information\n","glut\n",",\n","is\n","available\n","in\n","vast\n","quantities\n",".\n","A\n","better\n","strategy\n","will\n","generally\n","be\n","to\n","use\n","more\n","data\n","Then\n","the\n","difference\n","between\n","the\n","motivated\n","and\n","the\n","arbitrary\n","will\n","be\n","evident\n","without\n","the\n","use\n","of\n","compromised\n","hypothesis\n","testing\n",".\n","As\n","Lord\n","Rutherford\n","put\n","it\n",":\n","“\n","If\n","your\n","experiment\n","needs\n","statistics\n",",\n","you\n","ought\n","to\n","have\n","done\n","a\n","better\n","experi-\n","ment\n",".\n","”\n","Received\n","July\n","2004\n","Lexical\n","Computing\n","Ltd.\n","Revisions\n","received\n","May\n","2005\n","Final\n","acceptance\n","May\n","2005\n","Notes\n","*\n","This\n","work\n","was\n","supported\n","by\n","the\n","UK\n","EPSRC\n",",\n","under\n","grants\n","GR\n","/\n","K18931\n","(\n","SEAL\n",")\n","and\n","GR\n","/\n","M54971\n","(\n","WASPS\n",")\n",".\n","1\n",".\n","In\n","this\n","paper\n","we\n","do\n","not\n","consider\n","the\n","distinction\n","between\n","the\n","predictable\n","and\n","the\n","‘\n","merely\n","’\n","motivated\n",".\n","2\n",".\n","Diapers\n",",\n","in\n","American\n","English\n","3\n",".\n","There\n","is\n","some\n","confusion\n","over\n","names\n",".\n","In\n","information\n","theory\n",",\n","Mutual\n","Information\n","is\n","usually\n","defined\n","over\n","a\n","whole\n","population\n","of\n","words\n",",\n","rather\n","than\n","being\n","specified\n","for\n","a\n","particular\n","word\n","-\n","pair\n",",\n","as\n","here\n",",\n","and\n","the\n","definition\n","incorporates\n","information\n","from\n","all\n","cells\n","of\n","the\n","contingency\n","table\n",".\n","Church\n","and\n","Hanks\n","only\n","use\n","a\n","subset\n","of\n","that\n","information\n",".\n","Church\n","-\n","and\n","-\n","Hanks\n","Mutual\n","Information\n","has\n","been\n","called\n","Pointwise\n","Mutual\n","Information\n",".\n","see\n","Manning\n","and\n","Schütze\n","(\n","1999\n",":\n","66\n","ff\n",".\n",")\n","for\n","a\n","fuller\n","discus-\n","sion\n",".\n","Here\n","we\n","use\n","Church\n","and\n","Hanks\n","’s\n","definition\n","and\n","name\n",".\n","4\n",".\n","This\n","sentence\n","will\n","be\n","confusing\n","to\n","non\n","-\n","mathematicians\n",".\n","The\n","χ2\n","statistic\n","is\n","a\n","statis-\n","tic\n",",\n","that\n","is\n",",\n","it\n","can\n","be\n","calculated\n","from\n","a\n","data\n","sample\n","using\n","actual\n","numbers\n",".\n","The\n","χ2\n","distribution\n","is\n","a\n","theoretical\n","construct\n",".\n","If\n","a\n","sufficiently\n","large\n","number\n","of\n","chi\n","-\n","square\n","statistics\n","are\n","calculated\n",",\n","all\n","from\n","true\n","random\n","samples\n","of\n","the\n","same\n","population\n",",\n","then\n","this\n","population\n","of\n","χ2\n","statistics\n","will\n",",\n","provably\n",",\n","fit\n","a\n","χ2\n","distribution\n",".\n","This\n","is\n","also\n","true\n","for\n","other\n","statistics\n",":\n","that\n","is\n",",\n","if\n","a\n","sufficiently\n","large\n","number\n","of\n","log\n","-\n","likelihood\n","statistics\n","are\n","calculated\n",",\n","all\n","from\n","true\n","random\n","samples\n","of\n","the\n","same\n","population\n",",\n","then\n","this\n","population\n","of\n","log\n","-\n","likelihood\n","statistics\n","will\n",",\n","provably\n",",\n","fit\n","a\n","χ2\n","distribution\n",".\n","Some\n","texts\n","call\n","the\n","statistic\n","χ2\n","rather\n","than\n","χ2\n","to\n","distinguish\n","it\n","more\n","clearly\n","from\n","the\n","distribution\n",",\n","but\n","this\n","practice\n","is\n","in\n","the\n","minority\n","and\n","is\n","not\n","adopted\n","here\n",".\n","5\n",".\n","See\n","appendix\n","6\n",".\n","The\n","model\n","used\n","was\n","a\n","sophisticated\n","one\n","incorporating\n","evidence\n","about\n","type\n","fre-\n","quencies\n","of\n","verbs\n","from\n","the\n","ANLT\n","lexicon\n",":\n","see\n","Briscoe\n","and\n","Carroll\n","(\n","1997\n",")\n","or\n","Kor-\n","honen\n",",\n","Correll\n",",\n","and\n","McCarthy\n","(\n","2000\n",")\n","for\n","details\n",".\n","7\n",".\n","See\n","Kilgarriff\n","and\n","Grefenstette\n","(\n","2003\n",")\n","and\n","papers\n","therein\n",".\n","The\n","web\n","is\n","a\n","vast\n","re-\n","source\n","for\n","many\n","languages\n",".\n","See\n","also\n","Banko\n","and\n","Brill\n","(\n","2001\n",")\n","for\n","the\n","benefits\n","of\n","large\n","data\n","over\n","sophisticated\n","mathematics\n",".\n","\f\n","274\n","A.\n","Kilgarriff\n","Appendix\n","The\n","average\n","value\n","of\n","the\n","error\n","term\n","is\n","0.5\n",".\n","We\n","explain\n","this\n","as\n","follows\n",".\n","If\n","we\n","do\n","in\n","fact\n","have\n","a\n","random\n","distribution\n",",\n","then\n","by\n","the\n","definition\n","of\n","the\n","χ2\n","distribution\n",",\n","the\n","sum\n","of\n","the\n","cells\n","in\n","the\n","contingency\n","table\n","is\n","1\n",":\n","a⫹b⫹c⫹d⫽1\n","Each\n","of\n","these\n","error\n","terms\n","is\n","calculated\n","as\n","(\n","O\n","⫺\n","E\n","⫺\n","0.5\n",")\n","2\n","/\n","E\n","In\n","our\n","situation\n",",\n","there\n","are\n","very\n","large\n","datasets\n","and\n","the\n","phenomenon\n","of\n","interest\n","only\n","accounts\n","for\n","a\n","very\n","small\n","proportion\n","of\n","cases\n",".\n","The\n","fre-\n","quency\n","of\n","not\n","word\n","w\n","is\n","very\n","high\n",".\n","Thus\n","the\n","expected\n","values\n",",\n","E\n",",\n","for\n","not\n","word\n","w\n","to\n","be\n","used\n","when\n","calculating\n","c\n","and\n","d\n","for\n","the\n","contingency\n","table\n","are\n","very\n","high\n",".\n","As\n","we\n","divide\n","by\n","very\n","large\n","E\n",",\n","c\n","and\n","d\n","are\n","vanishingly\n","small\n",",\n","so\n","a⫹b⫹c⫹d⫽1\n","reduces\n","to\n","a⫹b⫽1\n","Since\n","we\n","have\n","set\n","the\n","situation\n","up\n","symmetrically\n",",\n","a\n","and\n","b\n","are\n","the\n","same\n","size\n",",\n","so\n","each\n","will\n","be\n",",\n","on\n","average\n",",\n","0.5\n",".\n","References\n","Banko\n",",\n","Michele\n","and\n","Eric\n","Brill\n","2001\n","Scaling\n","to\n","very\n","very\n","large\n","corpora\n","for\n","natural\n","language\n","disambiguation\n",".\n","Proceedings\n","of\n","the\n","39th\n","Annual\n","Meeting\n","of\n","the\n","Association\n","for\n","Computa-\n","tional\n","Linguistics\n","and\n","the\n","10th\n","Conference\n","of\n","the\n","European\n","Chapter\n","of\n","the\n","Association\n","for\n","Computational\n","Linguistics\n",".\n","Bod\n",",\n","Rens\n","1995\n","Enriching\n","linguistics\n","with\n","statistics\n",":\n","performance\n","models\n","of\n","natural\n","lan-\n","guage\n",".\n","Ph.D.\n","dissertation\n",",\n","University\n","of\n","Amsterdam\n",".\n","Brandstätter\n",",\n","E.\n","1999\n","Confidence\n","intervals\n","as\n","an\n","alternative\n","to\n","significance\n","testing\n",".\n","Methods\n","of\n","Psychological\n","Research\n","Outline\n","4(2\n",")\n",",\n","33⫺46\n",".\n","Brent\n",",\n","Michael\n","R.\n","1993\n","From\n","grammar\n","to\n","lexicon\n",":\n","unsupervised\n","learning\n","of\n","lexical\n","syntax\n",".\n","Com-\n","putational\n","Linguistics\n","19(2\n",")\n",",\n","243⫺262\n",".\n","Briscoe\n",",\n","Ted\n","and\n","John\n","Carroll\n","1997\n","Automatic\n","extraction\n","of\n","subcategorization\n","from\n","corpora\n",".\n","Proceedings\n","of\n","the\n","Fifth\n","Conference\n","on\n","Applied\n","Natural\n","Language\n","Processing\n",",\n","356⫺363\n",".\n","\f \n","Language\n","is\n","never\n",",\n","ever\n",",\n","ever\n",",\n","random\n","275\n","Carver\n",",\n","R.\n","P.\n","1993\n","The\n","case\n","against\n","statistical\n","significance\n","testing\n",",\n","revisited\n",".\n","Journal\n","of\n","Ex-\n","perimental\n","Education\n","61\n",",\n","287⫺292\n",".\n","Church\n",",\n","Kenneth\n","and\n","Patrick\n","Hanks\n","1990\n","Word\n","association\n","norms\n",",\n","mutual\n","information\n","and\n","lexicography\n",".\n","Compu-\n","tational\n","Linguistics\n","16(1\n",")\n",",\n","22⫺29\n",".\n","Dunning\n",",\n","Ted\n","1993\n","Accurate\n","methods\n","for\n","the\n","statistics\n","of\n","surprise\n","and\n","coincidence\n",".\n","Compu-\n","tational\n","Linguistics\n","19(1\n",")\n",",\n","61⫺74\n",".\n","Gale\n",",\n","William\n","and\n","Geoffrey\n","Sampson\n","1995\n","Good\n","-\n","Turing\n","frequency\n","estimation\n","without\n","tears\n",".\n","Journal\n","of\n","Quantitative\n","Linguistics\n","2(3\n",")\n",",\n","Good\n",",\n","I.\n","J.\n","1953\n","The\n","population\n","frequencies\n","of\n","species\n","and\n","the\n","estimation\n","of\n","population\n","parameters\n",".\n","Biometrika\n","40\n",",\n","237⫺264\n",".\n","Grefenstette\n",",\n","Gregory\n","and\n","Julien\n","Nioche\n","2000\n","Estimation\n","of\n","English\n","and\n","non\n","-\n","English\n","language\n","use\n","on\n","the\n","www\n",".\n","In\n","Proceedings\n","of\n","RIAO\n","(\n","Recherche\n","d’Informations\n","Assiste\n","´\n","e\n","par\n","Ordinateur\n",")\n",",\n","237⫺246\n",".\n","Hofland\n",",\n","Knud\n","and\n","Stig\n","Johanson\n","(\n","Eds\n",".\n",")\n","1982\n","Word\n","Frequencies\n","in\n","British\n","and\n","American\n","English\n",".\n","Bergen\n",":\n","The\n","Norwe-\n","gian\n","Computing\n","Centre\n","for\n","the\n","Humanities\n",".\n","Korhonen\n",",\n","Anna\n","2000\n","Using\n","semantically\n","motivated\n","estimates\n","to\n","help\n","subcategorization\n","acquisition\n",".\n","Proceedings\n","of\n","the\n","Joint\n","Conference\n","on\n","Empirical\n","Methods\n","in\n","NLP\n","and\n","Very\n","Large\n","Corpora\n",",\n","216⫺223\n",".\n","Korhonen\n",",\n","Anna\n",",\n","Genevieve\n","Gorrell\n",",\n","and\n","Diana\n","McCarthy\n","2000\n","Statistical\n","filtering\n","and\n","subcategorization\n","frame\n","acquisition\n",".\n","Proceedings\n","of\n","the\n","Joint\n","Conference\n","on\n","Empirical\n","Methods\n","in\n","NLP\n","and\n","Very\n","Large\n","Corpora\n",",\n","199⫺206\n",".\n","LDOCE\n","1995\n","Longman\n","Dictionary\n","of\n","Contemporary\n","English\n",",\n","3rd\n","Edition\n",".\n","Ed\n",".\n","Della\n","Summers\n",".\n","Harlow\n",":\n","Longman\n",".\n","Leech\n",",\n","Geoffrey\n","and\n","Roger\n","Fallon\n","1992\n","Computer\n","corpora\n","—\n","what\n","do\n","they\n","tell\n","us\n","about\n","culture\n","?\n","ICAME\n","Journal\n","16\n",",\n","29⫺50\n",".\n","Manning\n",",\n","Christopher\n","and\n","Hinrich\n","Schütze\n","1999\n","Foundations\n","of\n","Statistical\n","Natural\n","Language\n","Processing\n",".\n","Cambridge\n",",\n","MA\n",":\n","MIT\n","Press\n",".\n","Owen\n",",\n","Frank\n","and\n","Ronald\n","Jones\n","1977\n","Statistics\n",".\n","Polytech\n","Publishers\n",".\n","Pedersen\n",",\n","Ted\n","1996\n","Fishing\n","for\n","exactness\n",".\n","Proceedings\n","of\n","the\n","Conference\n","of\n","the\n","South\n","-\n","Central\n","SAS\n","Users\n","Group\n",",\n","188⫺200\n",".\n","Rayson\n",",\n","Paul\n","and\n","Roger\n","Garside\n","2000\n","Comparing\n","corpora\n","using\n","frequency\n","profiling\n",".\n","Proceedings\n","of\n","the\n","Work-\n","shop\n","on\n","Comparing\n","Corpora\n",",\n","38th\n","ACL\n",",\n","1⫺6\n",".\n","Rayson\n",",\n","Paul\n",",\n","Geoffrey\n","Leech\n",",\n","and\n","Mary\n","Hodges\n","1997\n","Social\n","differentiation\n","in\n","the\n","use\n","of\n","English\n","vocabulary\n",":\n","some\n","analysis\n","of\n","the\n","conversational\n","component\n","of\n","the\n","British\n","National\n","Corpus\n",".\n","Interna-\n","tional\n","Journal\n","of\n","Corpus\n","Linguistics\n","2(1\n",")\n",",\n","133⫺152\n",".\n","Stubbs\n",",\n","Michael\n","1995\n","Collocations\n","and\n","semantic\n","profiles\n",":\n","On\n","the\n","cause\n","of\n","the\n","trouble\n","with\n","quantitative\n","studies\n",".\n","Functions\n","of\n","Language\n","2(1\n",")\n",",\n","23⫺55\n",".\n","\f\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Te5LTwa5Vzsa"},"id":"Te5LTwa5Vzsa","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}